This chapter seeks to answer six questions:
%
\begin{enumerate}[nosep]
 %
 \item How much developer effort and application modification does \sys require? (\S\ref{s:eval-effort})
%
\item How expensive are common application operations, as well as
  \xxing, revealing, and operations over \xxed data with \sys? (\S\ref{s:eval-ops})
 %

\item What overheads does \sys impose, and where do they come from?
    (\S\ref{s:eval-overheads})

\item 
    How does the effort required to implement \sys's
    functionality in a related system (Qapla~\cite{qapla}),
    and its performance, compare with using \sys?
    (\S\ref{s:eval-qapla})

\item 
    What is the performance impact of composing \sys's guarantees
    with those of encrypted databases?
        (\S\ref{s:eval-cryptdb})

\item 
    Which categories of global database updates can \sys
        support, and with what overheads? (\S\ref{s:eval-updates})
%
\end{enumerate}

%We compare Edna to three alternative settings:
To measure the \sys's expenses (\S\ref{s:eval-ops}), we compare \sys to a manual version of each
\xxing transformation that directly modifies the database (\eg via SQL queries
that remove data), which lacks support for revealing and does not support
composition of multiple transformations. 
%\two{} an implementation of disguising and revealing in
%Qapla~\cite{qapla} using
%Qaplaâ€™s query rewriting and access control policies; and \three{} an integration
%of Edna with CryptDB~\cite{cryptdb}, an encrypted database.

%
%To understand the cost of using \sys, we implemented a manual version of each
%\xxing transformation that directly modifies the database, akin to what
%developers might do today (\eg via SQL queries that remove data).
%%
%This manual baseline lacks support for revealing, and does not support
%composition of multiple transformations.
%%

%
%To evaluate the benefits of \sys's design, we compare it to
%Qapla~\cite{qapla}.
%%
%Qapla enforces access control policies by rewriting SQL queries,
%with predicates that restrict the rows returned,
%and we implemented \xxing and revealing functionality using these policies.
%
%With Qapla,
%developers can write policies that enable users to control visibility of their
%data.
%, and developers may want to use Qapla to implement user data controls.
%
%Finally, we combine \sys with CryptDB~\cite{cryptdb} and show that the two
%systems offer different, complementary guarantees.
%

All benchmarks run on a Google Cloud \texttt{n1-standard-16} instance with 16
CPUs and 60 GB RAM, running Ubuntu 20.04.5 LTS. Benchmarks run in a closed-loop
setting, so throughput and latency are inverses. %of each other.
%
The benchmarks use MariaDB 10.5 with the InnoDB storage engine atop a local SSD.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\sys Developer Effort}
\label{s:eval-effort}

% We estimate developer effort to use \sys via the lines of code of spec (JSON)
% and application code added to support \xxing and revealing transformations.

%Although developer effort is difficult to quantitively measure, there are
%clear effort benefits to using \sys over a manual approach, which we now engage with.
%
We evaluate the developer effort required to use \sys by measuring the
difficulty of implementing the \xxing and revealing transformations in our three
case studies.  This took one person-day per case study for a developer
familiar with \sys but unfamiliar with the applications.

A developer supporting these transformations must first add application
infrastructure to allow users to invoke them and notify users when they happen.
This is required even if the developer were to implement transformations
manually without \sys.
%Any support for these transformations---even if implemented manually without
%\sys---requires application changes to allow users to invoke them, and notify
%users when they happen.
%
%Support for \xxing and revealing transformations---even if implemented manually
%without \sys---necessarily require changes to application code. These changes
%add HTTP endpoints to invoke \xxing and revealing, and modifications to perform
%authorization checks for anonymous users and send emails to users with \xx IDs.
These changes add 179 LoC of Ruby to Lobsters (160k LoC), and 312 LoC of Rust
to the original WebSubmit (908 LoC). They implement HTTP endpoints,
authorization of anonymous users, and email notifications.
%

%
A developer using \sys also writes \xx specifications and invokes \sys.
Lobsters' \xx specifications are written in 518 LoC, WebSubmit's in 75 LoC, and
HotCRP's in 357 LoC (all in JSON).  The specification size is proportional to
the lines required to specify the database schema in SQL, as well as what data
each application \xxs.
%
Writing the corresponding updates for the three Lobsters global database
updates 
requires 81 LoC; this adds to the 89 LoC that the developer already
writes to implement the global database updates themselves.
%

Thus, the developer effort required to use \sys---writing \sys specifications,
and invoking \sys---requires adding <1k LoC per application (less than 1\% of
the code of real world applications like Lobsters), even though these
applications were not written with \sys in mind.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance of \sys Operations}
\label{s:eval-ops}

%
We next evaluate \sys's performance using WebSubmit,
HotCRP, and Lobsters (Ch.~\ref{ch:case_studies}).
%
%To measure the extra cost that \sys's cryptographic operations add,
%, as a developer would do without \sys.
%, and measure its latency.
%
We measure the latency of common operations, \xxing transformations, and
operations over \xxed data enabled by \sys (\eg account restoration and editing
\xxed data).
%
The three applications do not create new data that reference pseudoprincipals,
but to fully capture any overheads we configure \sys to nevertheless run the
checks for lingering pseudoprincipal references on revealing.
%

%
A good result for \sys would show no overhead on common operations, since \sys
should not be invoked on normal application execution paths.
%
\sys should be competitive with manual \xxing, with the caveat that \sys needs
to also encrypt and store disguised data.
%
Finally, \sys should have reasonable latencies for revealing operations
supported only by \sys (\eg a few seconds for account restoration), which
require both database queries and cryptographic operations. 
%impossible in the manual baseline
%

\begin{figure}
\begin{subfigure}[b]{\columnwidth}
    \centering
  \includegraphics[width=.8\columnwidth]{figs/websubmit_op_stats}
\caption{WebSubmit (2k users, 80 answers/user).}
\label{f:ops-websubmit}
\end{subfigure}

    \vspace{12pt}
\begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=.8\columnwidth]{figs/hotcrp_op_stats}
  \caption{HotCRP (80 reviewers, 3k total users, 200--300 records/reviewer).}
\label{f:ops-hotcrp}
\end{subfigure}

    \vspace{12pt}
\begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=.8\columnwidth]{figs/lobsters_op_stats}
\caption{Lobsters (16k users, Zipf-distributed data/user).}
\label{f:ops-lobsters}
\end{subfigure}
    \caption[Latency of common application, disguising, and
    revealing operations with \sys.]{\sys adds no latency overhead to common application operations and
modestly increases the latencies of \xxing operations compared to a manual
implementation that lacks support for revealing or composition.
%
Bars show medians, error bars are 5\textsuperscript{th}/95\textsuperscript{th}
percentile latencies.}
\label{fig:client_opstats}
\end{figure}

\textbf{WebSubmit.}
%
We run WebSubmit with a database of 2k users, 20 lectures with four questions
each, and an answer for each question for each user (160k total answers).
%We implement WebSubmit account removal and answer anonymization both manually
%as an irreversible database change, and with \sys.
%
We measure end-to-end latency to perform common application operations (which
each issue multiple SQL queries), as well as \xxing and revealing operations
when possible (revealing operations are impossible in the baseline).
%
%A good result for \sys would show competitive performance with the manual
%baseline, as \sys does strictly more work (\eg encrypting \xxed
%data).
%
%Latency includes request processing in WebSubmit and \sys's
%operations, and the response to the client.
%
Figure~\ref{f:ops-websubmit} shows that common operations have comparable
latencies with and without \sys.
%
\sys adds 9ms to account creation; \xxing and revealing operations also take
longer in \sys (13.2--40.0ms), but allow users to reveal their data.
%and take less developer effort.
%

\textbf{HotCRP.}
%
We measure server-side HotCRP operation latencies for PC members on a database seeded with 3,080
total users (80 PC members)
%and per-user data such as paper watches;
and 550 papers with eight reviews, three comments, and four conflicts each
(distributed evenly among the PC).
%
HotCRP supports the same \xxing transformations as WebSubmit, but PC users have more
data (200--300 records each), and HotCRP's \xxing transformations mix deletions and
decorrelations across 12 tables. %, so we would expect higher latencies.
%

%
Figure~\ref{f:ops-hotcrp} shows higher latencies than in WebSubmit in general, even for the manual
baseline, which reflects the more complex \xxing transformations.
%
\sys takes 64.4--79.5ms to \xx and reveal a PC member's data, again owing to the
cryptographic operations necessary.
%
HotCRP's account anonymization is admin-applied and runs for all PC members, so
its total latency is proportional to the PC size.
%
With 80 PC members, this transformation takes 6.4s, which is acceptable for a
one-off operation.
%
As before, \sys adds small latency to common application operations, and 9ms to
account creation.
%

\textbf{Lobsters.}
%
We run Lobsters benchmarks on a database seeded with 16k users, and
120k stories and 300k comments with votes, comparable to the late-2022 size of
production Lobsters~\cite{lobsters}.
%
Content is distributed among users in a Zipf-like distribution according to
statistics from the actual Lobsters deployment~\cite{lobsters-data}, and 20\% of
each user's contributions are associated with the topic to anonymize.
%
The benchmark measures server-side latency of common operations and
\xxing/revealing transformations.
%
%Because Lobsters users have far more variable amounts of
%data, we expect higher variability in latencies.
%and we measure the account
%decay \xxing transformation (and subsequent restoration) in addition to GDPR-compliant
%account removal and restoration.
%
%Lobsters does not support editing decayed contributions.
%
%As the amount of data per user follows a Zipf-like distribution, we would
%expect \xxing some users to be more expensive than others.
%

%
The results are in Figure~\ref{f:ops-lobsters}.
%
The median latencies for entire-account removal or decay are small (9.9--13.8ms
for \sys, and 4.6--5.8ms for the baseline), since the median Lobsters user has
little data. Revealing \xxed accounts takes 17.1--22.0ms in the median.
%
Highly active users with lots of data raise the 95\textsuperscript{th}
percentile latency to $\approx$150ms for account removal and 150ms for account
restoration.
%
Topic anonymization touches less data and is faster than whole-account
transformations, taking 4.0ms and 13.5ms for the median user to respectively \xx
and reveal.
%

\textbf{Summary.}
%
\sys necessarily adds some latency compared to manual, irreversible data
removal, since it encrypts and stores \xxed data.
%
However, most \xxing transformations are fast enough to run interactively as
part of a web request.
%
Some global \xxing transformations---\eg HotCRP's conference anonymization over
many users---take several seconds, but an application can apply these
incrementally in the background, as in Lobsters account decay.
%
%We next break down these costs.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\sys Performance Drill-Down}
\label{s:eval-additional}
%\lyt{made this a subsubsection}

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\columnwidth]{figs/composition_legend}
    \begin{subfigure}[b]{.48\columnwidth}
        \includegraphics[width=\columnwidth]{figs/composition_stats_websubmit}
        \caption{WebSubmit}
        \label{f:comp-websubmit}
      \end{subfigure}
      \begin{subfigure}[b]{.48\columnwidth}
          \includegraphics[width=\columnwidth]{figs/composition_stats_hotcrp}
          \caption{HotCRP}
        \label{f:comp-hotcrp}
      \end{subfigure}
    \caption[Composing disguising transformations increases latency of
    disguise and reveal linear in the number of involved
    pseudoprincipals.]{Applying \xxing transformations to previously-decorrelated accounts
      increases latency linear in the number of pseudoprincipals involved.
      %to the amount of data touched, as \sys performs
      %3--5.5$\times$ and restoration by 5.7--10$\times$ as \sys
      %cryptographic operations linear in the number of pseudoprincipals
      %involved.
      Hatched lines indicate the proportion of cost attributed to
      cryptographic operations.}
    \label{f:composition}
\end{figure}

This section breaks down the cost of \sys's operations into the cost of database
operations and the cost of cryptographic operations.
%
%\sys's basic API performs some computation (which we expect to be quick) as
%well as two operations that could cause bottlenecks, namely data encryption
%and decryption and access to \sys's database.
%
%\sys's high-level API additionally performs queries on the application database.
%
%\lyt{Each disguise and reveal operation requires decoding of JSON specifications
%into \sys-specific Rust datatypes; this takes $\approx0.1$ms on average. This
%could be offloaded to registering each specification with \sys when the
%application starts, allowing \sys to decode them in advance.}
%
\sys's database operations are fast; in our prototype, they generally take
$0.2$--$0.3$ms but vary depending on the amount of data touched.
%
%Database query latency depends on the amount of data
%touched, but is normally $<$1ms in our case studies.
%
\sys's cryptographic operations are comparatively expensive.
%; Figure~\ref{f:opstats} shows their cost.
%
PBKDF2 hashing for private key management incurs a 8ms cost and affects account
registration and operations on \xxed data that reconstruct a user's private key;
this accounts for up to 79\% of these operations' cost when the operation issues
only a few database queries.
%
%This appears during registration (generating password-based credentials from the
%private key), as well as operations on \xxed data that reconstruct the private key
%(\ie editing or restoring \xxed data).
%
%Applications that only wish to support private keys as reveal credentials can tell
%\sys to skip PBKDF hashing and secret sharing, eliminating this cost.
%

Encryption and decryption incur baseline costs of 0.1ms and 0.02ms respectively;
their cost grows linearly with data size.
%
In the common case, \xxing or revealing data performs two cryptographic
operations: one to encrypt/decrypt the diff and speaks-for records, and one to
encrypt/decrypt the ID at which they are stored.
%

\sys also generates a new key for each pseudoprincipal created, which takes
0.2ms.
\sys's cryptography accounts for up to 35\%
%1--35\%
of the cost of \xxing/revealing operations such as account removal or
anonymization; this proportion decreases as the number of database
modifications made by a transformation increases.
%Disguising undisguised data performs a single encryption, and
%A new disguise or reveal operation requires a single encryption/decryption
%when a single disguise to the data.
%
When the application applies multiple \xxing transformations and \xxs the
data of pseudoprincipals, doing so may require several encryptions/decryptions.
%
We evaluate this cost next.
%

%
%\lyt{ADDED THIS.}
%%
%\sys's \xxing transformations (\eg remove account) incur costs proportional to
%the amount of data a user owns: decorrelating a user's per-lecture answers in
%WebSubmit, for example, requires retrieving the user's answers,  inserting
%per-lecture pseudoprincipals, and update queries (one per each of the 80
%answers) to rewrite answers to correlate to pseudoprincipals. This results in a
%total of $\approx$18ms simply to perform database modifications.
%%
%For each of these decorrelations, \sys produces speaks-for records that are then
%encrypted and persisted, accounting for the remaining 2ms costs.
%%
%
%The cost of \sys's revealing transformations (\eg restore account) can also be
%explained as a breakdown between database query overheads and decryption
%overheads.
%%
%Restoring removed answers first requires decrypting a user's diff tokens.
%%
%For each of a user's 80 answers, \sys performs three selection queries for
%consistency checks to ensure that reinsertion can correct proceed (0.4-0.5ms)
%and inserts the answer (0.2ms).
%%
%Total restoration of a user's answers thus takes $\approx$50ms; the remaining 1-2ms cost
%comes from clearing the diff and speaks-for tokens of the revealed
%transformation, which requires both a decryption and a database deletion query.
%%
%Wrapping all reveal queries in a transaction did not meaningfully improve
%performance.
%%
%

\subsection{Composing \Xxing Transformations.}
\label{s:eval-composition}
% \lyt{this is now under \sys op performance}.

%
To understand the overhead of composing transformations in \sys (\S\ref{s:composition}), we measure the
cost of composing account removal on top of
%from an application that already invoked
a prior \xxing transformation to anonymize and decorrelate all users' data.
%
We consider WebSubmit and HotCRP, and compare three setups: \one{} manual
account removal (as before); \two{} account removal and restoration
\emph{without} a prior anonymization \xxing transformation; and \three{} account
removal and restoration \emph{with} a prior anonymization \xxing transformation.
%

%
With prior anonymization, a subset of the user's data has already been
decorrelated when removal occurs, and removal therefore performs
per-pseudoprincipal encryptions of \xxed data with pseudoprincipals' public
keys. 
%Thus, the cost of removal should increase proportional to the number of
%pseudoprincipals created by the first anonymization transformation.
%by the first \xxing transformation, and \sys
%
%This requires individually encrypting data with each pseudoprincipal's
%public key.
%
Restoring the removed, anonymized account must then individually decrypt
pseudoprincipal records and restore them.
% to both restore and then clear them.
%
Hence, \xxing and revealing in the third setup should take time proportional to
the number of pseudoprincipals created by anonymization.
%

%
Figure~\ref{f:composition} shows the resulting latencies.
%
WebSubmit account removal and restoration latencies increase by $\approx$1ms per
pseudoprincipal (19.4ms and 22.4ms respectively); 50\% of this increased cost
comes from the additional, per-pseudoprincipal encryption and decryption of
records, the rest comes from database operations.

%
HotCRP removal latencies also increase by $\approx$1ms per pseudoprincipal
(205.6ms) and restoration latencies increase by $\approx$2ms per pseudoprincipal
(392.0ms). Again, cryptographic operations add $\approx$0.5ms per
pseudoprincipal, and the remaining cost increase comes from
per-pseudo\allowbreak principal
database queries and updates. Restoration requires more per-pseudoprincipal
queries to \eg perform consistency checks, resulting in a greater latency
increase than in removal. 
%querying-for and per-pseudopricnipal
%
Compared to accounts in WebSubmit, accounts in HotCRP have more data and
14--15$\times$ more pseudoprincipals after anonymization, which accounts for the
larger relative slowdown and the increased effect of per-pseudoprincipal
reveals.

WebSubmit and HotCRP do not create new references to pseudoprincipals after data
is disguised, but if they did, \sys would need to issue additional
per-pseudoprincipal queries to rewrite or remove these references (if configured
to do so).
%
%
%HotCRP particularly demonstrates how cryptographic operations grow comparatively
%more expensive: from 21\% of the cost to 41\% for account removal, and from 8\%
%to 25\% for account restoration.
%
%
%Restoring an account (\ie undoing a composed disguise) is more expensive than removing an
%account (\ie composing a disguise atop another) because
%decryption is more expensive than encryption (Figure~\ref{f:opstats}).
%

%
Importantly, \xxing latencies stabilize when \sys composes further \xxing
transformations: since cost is proportional to the number of pseudoprincipals
affected, latency does not grow once the application has maximally decorrelated
data (to one pseudoprincipal per record), as done by HotCRP anonymization.
%\lyt{XXX} HotCRP's anonymization
%performs maximal decorrelation, and the subsequent account removal demonstrates
%this maximal cost.
%


% %
% The experiment composed an interactive \xxing transformation (account removal) atop a
% non-interactive one (anonymization).
% %
% This benefits from the optimization for interactive \xxing transformations mentioned in
% \S\ref{s:composition}: the natural principal introduces locators for
% pseudoprincipal-owned bags that \sys forgot, which helps avoid a locator
% decryption on revealing the composed \xxing transformation.
% %
% Composing a non-interactive \xxing transformation requires this extra encryption/decryption
% as \sys store the locators; but the cost remains linear in the number of
% pseudoprincipals touched.
% %

\section{\sys Overheads}
\label{s:eval-overheads}

\sys adds both space and compute overheads to the application: \sys stores
disguised data and metadata, adding tables to the application DB; and \sys may
run disguising and revealing transformations simultaneously with normal
application operations, increasing the load on the system and affecting normal
application operation throughput.
%
This section
measures both of these overheads.

\subsection{Space Used By \sys.}

\begin{figure}[t]
\centering
\begin{tabular}{lcc}
    \textbf{Component} & \textbf{Pre-Delete (MB)} & \textbf{Post-Delete (MB)} \\
\hline
    App Tables & 261 & 290\\
    %Disguise Table (Mem) & 0 & 34.3\\
    %Encrypted Locators (Mem) & 0 & 1.9\\
    %Principal Metadata (Mem) & 1.8 & 16.5\\
    %Shares Metadata (Mem) & 2.0 & 2.0\\
    Disguise Table (Disk) & 0 & 35.1\\
    Deleted Principals Table (Disk) & 0 & 0.4\\
    Principals Table (Disk) & 2.6 & 14.1 \\
    User Shares Table (Disk) & 8.9 & 8.9\\
\end{tabular}
    \caption[Space overhead of \sys.]{The space usage of the Lobsters-\sys database increases by 87.5MB on disk (and 44MB
    in memory) after 10\% of 16k
    users remove their accounts with \sys.}
    \label{f:storage}
\end{figure}

%
To understand \sys's space footprint, we measure the size of all data stored
on disk by \sys before and after 10\% of users in Lobsters (1.6k users)
remove their accounts.
%
%
Cryptographic material adds overhead and each generated pseudoprincipal adds an
additional user to the application database; \sys also stores data for each
registered principal (a public key and a list of opaque indexes) as well as
encrypted records.
%
%Clients keep track of their credentials, but these are small and encoded
%in application-specific ways.
%

%
Table~\ref{f:storage} shows the increase in space used by \sys and the
application after \sys disguises 10\% of Lobsters users. \sys's storage
initially consumes 12 MB, which consists of entries in the principals table and
user shares table from the registration of all 16k users.
%
\sys's storage space grows to 58.5 MB after the users remove their accounts, and
the application database size increases from 261 MB to 290 MB (+11\%).
%
(\sys also caches this data using 44MB in memory.)
%
The space used is primarily proportional to the number of pseudoprincipals
produced: each pseudoprincipal requires storing an application database record,
a speaks-for record, and row in the principal table.
%
In this experiment, Lobsters produces 78.1k pseudoprincipals.
%
%Storing a private key is necessary if the application wants to further
%disguise decorrelated data, but avoidable if disguises cannot compose.
%
%In addition, each pseudoprincipal inserts a public key into \sys's
%metadata, adding $\approx$0.3 KB per pseudoprincipal and 24 MB combined,
%
%
\sys removes the public keys for the 1.6k removed principals---subtracting from
the principals table, but adding to the deleted principals table---and removes
the principals' database data to store as encrypted diff records in the disguise
table, which uses 2.2 MB.
%
%These diff records amount to a small overhead (2.2MB).
%
%2.2MB is 0.8\% of the original Lobsters database size of 261 MB.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Impact On Concurrent Application Use.}
\label{s:eval-conc}

%
%Web applications serve many concurrent users, most of whom are simply using the
%application.
%
%Occasionally, a user (or an admin, or a cron job) will initiate a \xxing
%transformation via \sys.
%
For \sys to be practical, the throughput and latency of normal application
requests by other users should be largely unaffected by \sys's \xxing and
revealing operations, even though \sys may modify the same tables touched by
normal application requests.
%
%If an expensive \xxing transformation---such as a popular Lobsters user removing their
%account---blocked application processing for all other users, developers would
%be rightly leery of \sys.
%

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/lobsters_concurrent_results}
    \caption[Impact of continuously disguising and revealing on
    application request throughput.]{Continuous \xxing/revealing operations in Lobsters
    have <7\% impact on application request throughput when disguising a
    random user; an extreme case of a heavy-hitter user with lots of data
    repeatedly \xxing and revealing causes up to 8\% drop in throughput without
    transactions, and up to 28.3\% drop in throughput with transactions.}
    \label{f:concurrent-lobsters}
\end{figure}


%
This section thus measures the impact of \sys's operations on other concurrent requests
in Lobsters.
%
In the experiment, a set of users make continuous requests to the
application that simulate normal use, while another distinct set of users
continuously remove and restore their accounts.
%
\sys applies \xxing transformations sequentially, so only one transformation
happens at a time.
%
% (This is realistic, as the UI can process later requests asynchronously; it also
% avoids \sys having to reason about overlapping data between two or more \xxing
% transformations.)
%
We measure the throughput of ``normal'' users' application operations, both
without \sys operations (the baseline) and with the application continuously
invoking \sys.
%
The Lobsters workload is based on request distributions in the real
Lobsters deployment~\cite{lobsters-data}.
% users read stories, load the frontpage,
% vote and comment, and post stories.
%

%
Since users' \xxing/revealing costs vary in Lobsters, we measure the
impact of \one{} randomly chosen users invoking account removal/restoration, and
\two{} the user with the most data continuously removing and restoring their
account (a worst-case scenario).
% , since this overlaps most with the data touched by normal users).
%
The results illustrate throughput in a low load scenario ($\approx$20\% CPU load),
and a high load scenario ($\approx$95\% CPU load).
%
Finally, we measure settings with and without a transaction for \sys
transformations.
%
%The latter makes sense if the application wishes to avoid exposing
%partially-\xxed data to other clients.
%
A good result for \sys would show little impact on normal operation throughput
when concurrent \xxing transformations occur.
%

%low cheap no txn 0 
%low cheap txn 0.8
%low exp no txn 4.4
%low exp txn 28.3
%
%high random no txn 4.2
%high random txn 7.2
%high exp no txn 7.7
%high exp txn 25.3

%
Figure~\ref{f:concurrent-lobsters} shows the results.
%
If a random user disguises and reveals their data (the common case), normal
operations are mostly unaffected by concurrent \xxing and revealing: throughput
drops $\le$4.2\% without transactions and $\le$7.2\% with transactions (at high
load).
%
This shows that \sys's \xxing and revealing transformations have acceptable
impact on other users' application experience in the common case.

%
Constantly \xxing and revealing the user with the most data (the worst-case
scenario) has a larger effect, with throughput reduced by up to $7.7$\% without
transactions and up to 28.3\% with transactions at both low and high load.
%5.8\% in low load, 2.6 high 13.3\% in low load, 16.3 high dropping by at most
%4.6\%---by concurrent \xxing of a random user (with and without transactions)
%and of the most expensive user (without transactions); \xxing the most
%expensive user with transactions does, however, impact performance by 11.9\%.
%%
%Under high load, the relative impacts are similar: the throughput when
%concurrently \xxing a random user (with and without transactions) and the most
%expensive user (without transactions) drops by $\le 4.4$\%; and when disguising
%the
%%most expensive user with transactions, drops by 17.1\%.
%
%\lyt{XXX Note: drop w/out txn is actually more for low load than high load.}
%10% low load
%
%The throughput of normal application requests under low load, with account
%removals and restorations of either random users or the most expensive user,
%remains relatively unaffected at 1.3k ops/s (decreasing by $<$100 ops/s, or
%6\%).
%%
%Account removal and restoration under high load results in a slight decrease
%(1.8\%) in throughput (transactions decrease throughput by 3.8\%), but
%throughput remains high ($>5$ops/ms).
%
This worst-case scenario disguises and reveals a user owning 1\% of all the data
in a database of 16k users, and demonstrates the upper bound on Ednaâ€™s
performance impacts when Edna runs large disguising and revealing transactions
continuously.  Because the queries within disguise and reveal transactions touch
16 tables, all which are commonly read by normal application operations, this
impact is expected: transactions lock any written tables, preventing
application operations from reading them until the transaction has completed.
%
With transactions, the benchmark completes hundreds more disguising and
revealing transformations than when run without transactions; this illustrates
how \sys's transactions, when touching large amounts of data, can block the
execution of normal application operations.

%and operations over \xxed data
%
%\ms{Did we lose a sentence about account removals under low load here? The following doesn't quite flow from the prior point}
%
%Account removals and restorations under low load take longer, with
%the expensive user's account removal and revealing taking 4.4 and 3.6 seconds
%respectively.
%

The latency of disguising operations depends on load: on average, disguising and
revealing transformations take <100ms.  The expensive user's account removal and
revealing take 2.7 and 5.4 seconds under high load. 
%\lyt{Note: these
%numbers are about the same (if not higher) when not using transactions.}
%
%Revealing takes significantly longer than disguising due to the many more
%queries required to perform consistency checks. 
%
This is acceptable: 50\% of data deletions at Facebook take five minutes or
longer to complete~\cite{delf}.
%

\begin{comment}
high load disguise 2697 2790 2884
high load restore 6199 6305 6407
high load disguise txn 2640 2726 2847
high load restore txn 5216 5436 5667
expensive disguise 12 16 21
expensive restore 97 233 370
exp txn delete 20 24 30
exp txn restore 98 232 366
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{07-eval-qapla}

\input{07-eval-cryptdb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Global Database Updates}
\label{s:eval-updates}

%
If developers perform global database updates, this both increases
developer effort and affects \sys's performance. This section answers the following
two questions:
\begin{enumerate}[nosep]
    \item What categories of global database update can \sys support, and how
        easily? 
    \item What are the performance impacts of invoking \sys's API to record an
        update, and applying updates during reveal?
\end{enumerate}

\subsection{Supporting Updates}
\begin{figure}
    \centering
    \begin{tabular}{m{0.2\columnwidth}|m{0.2\columnwidth}|m{0.2\columnwidth}|m{0.2\columnwidth}}
        \centering\textbf{Attribute} & \textbf{Normalize URL} & \textbf{Add Story Texts} &
        \textbf{Add Show Email} \\
        \hline
        Global Update & 68 LoC & 19 LoC & 2 LoC \\
        \hline
        Update Spec & $\varnothing$ & 54 LoC & 27 LoC \\
    \end{tabular}
    \caption[Code required to write reveal-time update specifications.]{Schema migrations that affect entire tables require
    writing separate reveal-time update specifications; however, updates like
    normalizing URLs that already apply per rows can be used as update
    specifications.}
    \label{tab:updates}
\end{figure}

Developers must already write their global updates today (\eg Lobsters
implements these updates as Ruby Active Record Migrations~\cite{ruby_arm}). 
%
However, depending on the type of update , developers may need to additionally
write a reveal-time update specification for \sys to capture that operation.
%
Operations that execute by affecting entire tables (\eg via an \texttt{ALTER
TABLE} command) require writing separate updates which take as input a set of
rows instead of an entire table.
%
However, global updates may already operate on a row-level, and thus
can be reused as an update specification to \sys.


%
Our Lobsters implementation implements the selected three global
updates in
Rust instead of Ruby Active Record Migration. Table~\ref{tab:updates} shows the
amount of code required per operation, as well as the additional code required
to add a corresponding reveal-time update specification to give \sys. We reuse
the global URL normalization update code for the reveal-time update
specification to \sys, since the global update changes rows one-by-one. Thus,
the update specification requires no additional code.
%
However, we implement the two schema migrations that respectively add
\texttt{show\_email} and add \texttt{story\_texts} as global table updates, and
thus write separate reveal-time updates that operate on individual rows, taking
54 and 27 LoC respectively.
%
%Thus, the developer needs to write either no additional code to support updates
%if the bulk operation code can be reused, or 
%less than 100 LoC.
%\todo{too vague? it seems a lot compared to the table sql statements, but
%updates are rather straightforward.}
%
Invoking \sys with the three different global database updates for Lobsters
required adding one line of code per update that the application calls when
performing the update. 
%

\begin{figure}
    \centering
    \begin{tabular}{m{0.15\columnwidth}|m{0.05\columnwidth}|m{0.22\columnwidth}|m{0.43\columnwidth}}
        \centering\textbf{Update Type} & \centering\textbf{\%} &
        \centering\textbf{Examples} & \textbf{Example SQL} \\
        \hline
        Table & \begin{center}75\end{center} & Create tables \newline Alter table columns \newline Add table indexes & \texttt{ALTER TABLE users ADD COLUMN show\_email} \\
        \hline
            Row& \begin{center}15\end{center} & Moderate text \newline Normalize URLs & \texttt{UPDATE stories SET
        url= NORMALIZE(url)} \\
        \hline
            Unsupported & \begin{center}10\end{center} & Update cached story vote count & \texttt{UPDATE stories SET
        vote\_count=(SELECT COUNT(*) FROM votes WHERE votes.story\_id =
        stories.id)} \\
    \end{tabular}
 
    \caption[Survey of global database updates supported by \sys.]{Out of the most recent 20 Lobsters global database updates,
    the majority (75\%) are table operations which require developers to write
    separate update specifications for \sys. 15\% are row operations which can
    be reused as updates without additional effort, and 10\% are unsupported
    because they denormalize the schema in order to cache aggregate results.}
  \label{tab:categories}
\end{figure}

%
In addition to the updates described and implemented in
\S\ref{s:casestudies:updates}, we inspected the 20 most recent
global database updates in
Lobsters from the past three years, and classified them as:
\begin{enumerate}[nosep]
%
    \item a row operation (which can be reused as an update specification to \sys), such as URL
normalization; 
%
\item a table operation (which requires writing a new update specification for \sys), such as
generating tables from other tables (\eg texttt{story\_texts} from
\texttt{stories}) or altering columns; or
%
\item an unsupported operation (which cannot be supported as an update
    specification to 
    \sys), such as updating the \texttt{vote\_count} of comments based on the number of votes).
\end{enumerate}
Figure~\ref{tab:categories} shows the distribution of 20 global updates among these
three categories. 
%
\sys can support all operations other than updating rows based on current
external database state, unless the update's output is correct no matter the
state of other database tables. In the investigated Lobsters global database
updates, two operations out of the 20 were unsupported, and both updated the
cached \texttt{vote\_count} value of stories or comments.
%This represents 2 of the 20 inspected updates.
%
%
%As described in \S\ref{s:design:limits}, unless it is valid application behavior
%for \eg \texttt{vote\_count} to be 0 regardless of the number of votes, \sys
%will not correctly apply an update that sets the count of votes for comments.
%
These types of operations might occur in applications that optimize for
performance by denormalizing their database schema (as did Lobsters); we
hypothesize that this category of update would not exist with a normalized
schema.
%
However, these particular operations also happen to be idempotent for any rows
affected, and thus could be reapplied via a cron job regularly to update
revealed data.
%

%
For the large majority of global database updates, users can still reveal
their data disguised prior to these migrations, as if the migration had occurred
with their data present in the database.

%
\begin{comment}
    *row operation
    normalize URL
    set column value to NULL
    fix note formatting

    *table operation
    1 remove 
    add index and column
    add column
    add index
    add story_texts from stories
    create story_texts 
    add index
    add column (and modify it)
    create table
    set value and remove column
    rename column
    add column
    create categories and update columns
    add index
    add index

    *unsupported
    update comments based on count of votes from votes
    memoize score not upvotes

\end{comment}
%%%%%%%%%%%%%%%%
\subsection{Performance of Reveals with Updates}
\label{s:eval:updates}

\begin{figure}
    \centering
    \begin{tabular}{m{0.2\columnwidth}|m{0.2\columnwidth}|m{0.2\columnwidth}|m{0.2\columnwidth}}
        \centering\textbf{Attribute} & \textbf{Normalize URL} & \textbf{Add Story Texts} &
        \textbf{Add Show Email} \\
        \hline
        Global Update & 5.63s & 5.24s & 0.082s\\
        \hline
        Record Update & 0.82ms & 0.49ms & 0.43ms \\
        \hline
        Update 1 story & 0.22ms & 0.01ms & 0.01ms \\
        \hline
        Update 1 user & <0.01ms & <0.01ms & 0.01ms \\
        \hline
        Update other row & <0.001ms & <0.001ms & <0.001ms \\
    \end{tabular}
    \caption[Overheads of global updates and reveal-time updates.]{Applying the various updates adds greater overheads per story than
    per user or per other table row, as more updates apply to the stories table.
    URL normalization is the most expensive update because initialization of a
    URL object takes 0.2ms.}
    \label{tab:updates-perf}
\end{figure}

This section looks at how global updates affect application performance, and how
logging reveal-time update specs affects \sys's reveal performance.
Table~\ref{tab:updates-perf} shows the results.
%
Overall, the effects of performing updates increases with the amount of
disguised data a user wants to reveal.
%
Users with little data (\eg less than 5 stories) experience no visible increase
in reveal latency with updates, but users with lots of data can experience a
small effect (\eg revealing a user with 673 stories, 1971 comments, and 2000
messages takes on average 4.67s instead of 4.50s, an increase of 3.8\%).
%
The rest of this section breaks down these costs.
%

\paragraph{Global Update Performance.}
First, as a baseline, this section looks at the cost of performing
global updates, which
applications incur without \sys (Table~\ref{tab:updates-perf}, line 1).
%

%
The global URL normalization update takes 5.63s, the longest of all
updates.
%
This matches the optimized global update execution time of 6s in the deployed
Lobsters application, which normalizes the URLs of all stories in one
batch.\footnote{The Lobsters developers first implemented an unoptimized version
of URL normalization that normalizes stories one-by-one---this implementation
took 1789s.}
%
URL normalization modifies all 120k stories.
%

%
Adding the \texttt{show\_email} attribute to the \texttt{users} table takes
82ms, as it performs only one query to change the database schema.
%

%
Finally, creating and populating the \texttt{story\_texts} table and removing a
column from the \texttt{stories} table takes 5.24s.
%
This modifies all 120k stories, and changes the database schema by adding tables
and removing columns.
%

%
\paragraph{Record Update.}
%
Invoking \sys's hook to insert a new update specification in \sys's replay log
takes on average 0.58ms (Table~\ref{tab:updates-perf}, line 2), a small 
amount compared to the time to perform the update/migration This includes the
time to persist the update (one database insert query) and add it to \sys's
in-memory update replay log.
%

%
\paragraph{Reveal with Updates.}
%
This section evaluates the latency of reveal operations in Lobsters when updates
corresponding to the implemented three global updates have been applied after
reveal. \sys applies update specifications to data to reveal in batch, but the cost of
applying updates to revealed data should still be proportional to the amount of
data to reveal.

%\paragraph{Apply Update to Diff Record.}
First, we measure the cost to apply the various updates to diff record rows.
%
Updating \texttt{stories} diff records is the most costly, as both URL
normalization and the \texttt{story\_texts} updates apply: this takes 0.2--0.3ms
per story (Table~\ref{tab:updates-perf}, line 3).
%
Diff records for modified or decorrelated stories have both placeholder and
original rows. \sys applies updates to both placeholder and original rows, and
thus updates take 0.5--0.6ms per \texttt{stories} modify or decorrelate diff
record.
%
Initializing a URL normalizer object during the update takes a one-time cost of
0.2ms.
%
Because \sys applies updates to batches of rows of the same table
(\S\ref{s:impl:batching}), \sys amortizes the cost of URL normalizer
initialization by creating a URL normalizer object only once per reveal (when
\sys reveals all stories).
%

%
Applying all three updates to \texttt{users} diff records incurs $<0.1$ms per
user: the \texttt{show\_email} update appends a single column to user rows
(Table~\ref{tab:updates-perf}, line 4).
%

%
Although the three implemented updates do not affect other row types, the
implementation of Lobsters' updates does iterate through all rows to check
whether to update them. Each row iteration takes $\approx$1$\mu$
(Table~\ref{tab:updates-perf}, line 5). At scale (\eg for a user with thousands
of comments), this cost can add up to a couple milliseconds over the entire
reveal.
%

%\paragraph{Restore Updated Diff Record Rows.}
Finally, we measure the cost to restore updated rows to the database compared to
the cost to restore rows without updates.\footnote{Although restoring updated
rows is more expensive than restoring the original rows, the application's
global updates would have incurred the cost of updating these rows had the rows not
been disguised at the time. Thus, \sys moves the cost of updating these rows to
reveal time, rather than global updates execution time (although the cost may be higher
because the update is not batched over all table rows).}
%
The updates do not affect the cost of restoring rows for all tables other than
\texttt{stories}, since the queries to restore the rows are the same.  However,
restoring a story now requires additional queries to perform consistency checks
for and insert \texttt{story\_texts} table rows, adding $\approx$0.6ms per
story.
%

%
\section{Summary} 
This evaluation used \sys to add seven \xxing transformations to three web
applications. The benchmarks show that the effort required was reasonable, that
\sys's \xxing and revealing operations are fast enough to be practical, and that
they impose little to moderate overheads on normal application operation
depending on the amount of data being disguised or revealed.
%
